
# MLLM Research Repository

ë³¸ ë¦¬í¬ì§€í† ë¦¬ëŠ” Multimodal Large Language Models (MLLMs)ì— ëŒ€í•œ í•µì‹¬ ì—°êµ¬ ë…¼ë¬¸, í•™ìŠµ ë°ì´í„°ì…‹, ë²¤ì¹˜ë§ˆí¬ ì •ë³´ë¥¼ ì •ë¦¬í•œ ìë£Œì…ë‹ˆë‹¤.

---

## ğŸ“„ Contents

- [1. Key Papers](#1-key-papers)
- [2. Training Datasets](#2-training-datasets)
- [3. Benchmarks](#3-benchmarks)
- [4. Leaderboards](#4-leaderboards)
- [5. Related Open-Source Projects](#5-related-open-source-projects)
- [6. References](#6-references)

---

## 1. Key Papers

| Year | Title | Model | Institution | Link |
|------|-------|-------|-------------|------|
| 2023 | LLaVA: Visual Instruction Tuning | LLaVA | Microsoft, UIUC | [arXiv](https://arxiv.org/abs/2304.08485) |
| 2023 | MiniGPT-4 | MiniGPT-4 | Vision-CAIR | [arXiv](https://arxiv.org/abs/2304.10592) |
| 2024 | mPLUG-Owl2 | mPLUG-Owl2 | DAMO Academy | [arXiv](https://arxiv.org/abs/2308.12966) |
| 2023 | Kosmos-2 | Kosmos | Microsoft | [arXiv](https://arxiv.org/abs/2306.14824) |
| 2024 | Gemini 1.5 | Gemini | Google DeepMind | [blog](https://deepmind.google/technologies/gemini/gemini-15/) |

---

## 2. Training Datasets for Document Understanding

ì•„ë˜ í‘œëŠ” MLLMì˜ ì‚¬ì „í•™ìŠµ(Pre-training) ë° ëª…ë ¹ì–´ íŠœë‹(Instruction Tuning)ì„ ìœ„í•´ ì‚¬ìš©ëœ ì£¼ìš” ë°ì´í„°ì…‹ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤. ê° ë°ì´í„°ì…‹ì˜ ëª¨ë‹¬ë¦¬í‹°, ì„¤ëª…, í¬ê¸°, ì¶œì²˜, ì‚¬ìš© ë‹¨ê³„ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

| Dataset       | Description                                                   | Size           | Link                                                                                 |
|:--------------|:--------------------------------------------------------------|:---------------|:-------------------------------------------------------------------------------------|
| CCpdf         | ì›¹ì—ì„œ ìˆ˜ì§‘ëœ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¤êµ­ì–´ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹          | Unknown        | https://github.com/applicaai/CCpdf                                                  |
| ChartQA(train) | ì°¨íŠ¸ ì´ë¯¸ì§€ë¡œë¶€í„° ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ì§ˆë¬¸ì— ë‹µí•˜ëŠ” QA ë°ì´í„°ì…‹          | 28,299 QA pairs / 18,317 images | https://huggingface.co/datasets/ahmed-masry/ChartQA                               |
| DeepForm      | ë¬¸ì„œ í¼ ì¸ì‹ì„ ìœ„í•œ ì‹œê°ì -í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹                    | Unknown        | https://github.com/NVIDIA/DeepForm                                                 |
| DocVQA | ë¬¸ì„œ ì´ë¯¸ì§€ë¡œë¶€í„° QAë¥¼ ìˆ˜í–‰í•˜ëŠ” ì‹œê°ì  ë¬¸ì„œì´í•´ ë°ì´í„°ì…‹. | Train: 39,463 QA / 10,194 img<br>Val: 5,349 QA / 1,286 img<br>Test: 5,188 QA / 1,287 img | https://www.docvqa.org/datasets/docvqa |
| DVQA          | ë§‰ëŒ€ ì°¨íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìë™ ì§ˆë¬¸ì‘ë‹µ ë°ì´í„°ì…‹                | Train: 2,325,316 QA / 200,000 img<br>Test-Familiar: 580,557 QA / 50,000 img<br>Test-Novel: 581,321 QA / 50,000 img           | https://github.com/kushalkafle/DVQA_dataset?tab=readme-ov-file                                       |
| FigureQA      | ë„í˜• ì°¨íŠ¸ë¥¼ í™œìš©í•œ ì‹œê°ì  QAìš© í•©ì„± ë°ì´í„°                    | Train: 1,300,000 QA / 100,000 img<br>Test-Familiar: 250,000 QA / 20,000 img<br>Test-Novel: 250,000 QA / 20,000 img          | https://www.microsoft.com/en-us/research/project/figureqa-dataset/downloads/                                              |
| InfoVQA       | ì¸í¬ê·¸ë˜í”½ ê¸°ë°˜ ë³µì¡í•œ ì‹œê° ì •ë³´ ì§ˆì˜ì‘ë‹µ                     | Train: 23946 QA / 4406 img<br>Val: 2801 QA / 500 img<br>Test: 3288 QA / 579 img        | https://github.com/Chan1121/InfoVQA                                                |
| KLC           | ì°¨íŠ¸ ì´ë¯¸ì§€ì™€ ëŒ€ì‘í•˜ëŠ” ì„¤ëª… ìƒì„± ë° QA                        | Unknown        | https://github.com/salesforce/KLC                                                  |
| OCRVQA        | OCR ê²°ê³¼ì™€ ê²°í•©ëœ QAìš© ì‹œê°ë¬¸ì„œ ë°ì´í„°ì…‹                      | 1M             | https://ocr-vqa.github.io/                                     |
| PlotQA        | í”Œë¡¯ ì´ë¯¸ì§€ë¡œë¶€í„° ì •ë³´ ì¶”ì¶œì„ ìœ„í•œ QA                         | Train: 20,249,479 QA / 157,070 img<br>Val: 4,360,648 QA / 33,650 img<br>Test: 4,342,514 QA / 33,657 img           | https://github.com/NiteshMethani/PlotQA                                                 |
| PubTabNet     | í‘œ êµ¬ì¡°ë¥¼ ì¸ì‹í•˜ê³  ì¬êµ¬ì„±í•˜ê¸° ìœ„í•œ OCR ë°ì´í„°                 | Unknown           | https://github.com/ibm-aur-nlp/PubTabNet                                           |
| RVL-CDIP      | ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ìŠ¤ìº” ë¬¸ì„œ                        | 400K           | https://huggingface.co/datasets/aharley/rvl_cdip                                          |
| SynthDog      | ë¬¸ì„œ OCRì„ ìœ„í•œ í•©ì„± ë°ì´í„°ì…‹(ì˜ì–´)                                 | 0.5M        | https://huggingface.co/datasets/naver-clova-ix/synthdog-en                                                |
| SynthDog-ko      | ë¬¸ì„œ OCRì„ ìœ„í•œ í•©ì„± ë°ì´í„°ì…‹(í•œêµ­ì–´)                                 | 0.5M        | https://huggingface.co/datasets/naver-clova-ix/synthdog-ko                                                |
| TAT-QA        | í‘œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì¹˜ì  ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ì…‹      | 16K            | https://nextplusplus.github.io/TAT-QA/                                            |
| TAT-DQA        | ë¬¸ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì¹˜ì  ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ì…‹      | 16K            | https://nextplusplus.github.io/TAT-DQA/                                             |
| VisualMRC     | ë¬¸ì„œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹¤ì¤‘ëª¨ë‹¬ ë¨¸ì‹  ë…í•´                  | Unknown        | https://github.com/yoheikikuta/visualmrc                                           |
| WildReceipt   | ì˜ìˆ˜ì¦ ì´ë¯¸ì§€ì˜ OCR ë° ì •ë³´ ì¶”ì¶œ                              | 25K            | https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/WildReceipt |
| DocILE        | ë¬¸ì„œì˜ í•„ë“œ ì¸ì‹ ë° êµ¬ì¡°ì  íƒœê¹…ì„ ìœ„í•œ ë²¤ì¹˜ë§ˆí¬               | Unknown        | https://github.com/rossumai/docile                                                 |
| InsuranceQA   | ë³´í—˜ ê´€ë ¨ FAQì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹                        | 27K            | https://github.com/shuzi/insuranceQA                                               |
| OCR-TROSD     | í…ìŠ¤íŠ¸ ì¸ì‹ì„ ìœ„í•œ ì¤‘êµ­ì–´ OCR ë°ì´í„°                          | Unknown        | https://tianchi.aliyun.com/competition/entrance/531937/introduction                |
| WTQ           | ì›¹ í…Œì´ë¸”ì„ ì´ìš©í•œ ë³µì¡í•œ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹                   | 22K            | https://nlp.stanford.edu/software/sempre/#wtq                                      |



## 3. LLaVA ëª¨ë¸ í•™ìŠµ ë°ì´í„°

LLaVA (Large Language and Vision Assistant)ëŠ” ë©€í‹°ëª¨ë‹¬ ëª…ë ¹ì–´ ì´í•´ë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

### 1. Feature Alignment (ì‚¬ì „í•™ìŠµ) ë‹¨ê³„

- **ì‚¬ìš© ë°ì´í„°:** CC3M (Conceptual Captions 3M)ì—ì„œ í•„í„°ë§ëœ ì•½ 595K ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ
- **í•™ìŠµ ëª©ì :** ë¹„ì „ ì¸ì½”ë”ì˜ ì¶œë ¥ì„ ì–¸ì–´ ëª¨ë¸ì˜ ì„ë² ë”© ê³µê°„ì— ì •ë ¬í•˜ê¸° ìœ„í•œ íˆ¬ì˜ í–‰ë ¬ í•™ìŠµ
- **í•™ìŠµ ë°©ì‹:** ë¹„ì „ ì¸ì½”ë”ì™€ ì–¸ì–´ ëª¨ë¸ì€ ê³ ì •, íˆ¬ì˜ í–‰ë ¬ë§Œ í•™ìŠµ

### 2. Visual Instruction Tuning (ëª…ë ¹ì–´ íŠœë‹) ë‹¨ê³„

- **ì‚¬ìš© ë°ì´í„°:**
  - LLaVA-Instruct-150K: GPT-4ë¡œ ìƒì„±ëœ ì•½ 150K ê°œì˜ ì‹œê° ëª…ë ¹ì–´-ì‘ë‹µ ìŒ
  - ì¶”ê°€ì ìœ¼ë¡œ ì•½ 515K ê°œì˜ VQA ë°ì´í„° í¬í•¨
- **í•™ìŠµ ëª©ì :** ë‹¤ì–‘í•œ ì‹œê° ëª…ë ¹ì–´ì— ë”°ë¼ ì •ë°€í•œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ ê°•í™”
- **í•™ìŠµ ë°©ì‹:** ë¹„ì „ ì¸ì½”ë”ëŠ” ê³ ì •í•˜ê³ , íˆ¬ì˜ í–‰ë ¬ ë° ì–¸ì–´ ëª¨ë¸ì„ í•¨ê»˜ ë¯¸ì„¸ ì¡°ì •

**ì°¸ê³  ë§í¬:**
- GitHub: [haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)
- ë…¼ë¬¸: [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)

## 4. Gemma ëª¨ë¸ í•™ìŠµ í† í° ìš”ì•½

Gemma 3 ì‹œë¦¬ì¦ˆëŠ” ëª¨ë¸ ê·œëª¨ì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ ì–‘ì˜ í† í°ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ê°€ì¥ í° **Gemma 3 27B** ëª¨ë¸ì€ ì´ **14ì¡°(14T)** í† í°ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìœ¼ë©°, **Gemma 3 12B** ëª¨ë¸ì€ **12ì¡°(12T)** í† í°, **Gemma 3 4B** ëª¨ë¸ì€ **4ì¡°(4T)** í† í°, **Gemma 3 1B** ëª¨ë¸ì€ **2ì¡°(2T)** í† í°ìœ¼ë¡œ ê°ê° í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì°¨ë“±ì  í•™ìŠµ í† í° ì„¤ê³„ëŠ” ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ í•™ìŠµ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ê· í˜•ì„ ê³ ë ¤í•œ êµ¬ì¡°ì…ë‹ˆë‹¤.

---

## 5. Benchmarks

| Benchmark | Description | Task Type | Link |
|-----------|-------------|-----------|------|
| MMMU | Multimodal Math & Science Reasoning | Multi-choice QA | [MMMU](https://mmmu-benchmark.github.io/) |
| MathVista | Math Diagrams + QA | Diagram QA | [MathVista](https://mathvista.github.io/) |
| ChartQA | Chart Image Comprehension | Visual QA | [ChartQA](https://github.com/vis-nlp/ChartQA) |
| ScienceQA | Multimodal Science QA | Multichoice Reasoning | [ScienceQA](https://github.com/lupantech/ScienceQA) |
| MathBench | Text + Diagram + Equation Reasoning | Mixed | [MathBench](https://mathbench.github.io/) |

---

## 6. Leaderboards

- [LLaVA Leaderboard](https://llava-vl.github.io/)
- [MMMU Leaderboard](https://mmmu-benchmark.github.io/)
- [ChartQA Leaderboard](https://chartqa.github.io/)
- [HuggingFace Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open-llm-leaderboard)

---

## 7. Related Open-Source Projects

- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)
- [mPLUG-Owl2](https://github.com/X-PLUG/mPLUG-Owl)
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)
- [GPT4-Vision-Adapter](https://github.com/Adapter-Hub/GPT4-Vision-Adapter)
- [Gemma](https://github.com/google/gemma)
- [DocKylin](https://github.com/ZZZHANG-jx/DocKylin)

---

## 8. References

- Papers with Code: [https://paperswithcode.com/](https://paperswithcode.com/)
- HuggingFace Model Zoo: [https://huggingface.co/models](https://huggingface.co/models)
- ArXiv MLLM Trackers: [https://www.arxiv-sanity.com/](https://www.arxiv-sanity.com/)

---

ğŸ’¡ *ì´ ì €ì¥ì†ŒëŠ” ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.*

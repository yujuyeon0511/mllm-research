# 📘 MLLM Research Repository

본 리포지토리는 Multimodal Large Language Models (MLLMs)에 대한 핵심 연구 논문, 학습 데이터셋, 벤치마크 정보를 정리한 자료입니다. 연구자, 개발자, 실무자가 최신 멀티모달 AI 기술을 빠르게 이해하고 응용할 수 있도록 구성되어 있습니다.

---

## 📄 Contents

- [1. Key Papers](#1-key-papers)
- [2. Training Datasets](#2-training-datasets)
- [3. Benchmarks](#3-benchmarks)
- [4. Leaderboards](#4-leaderboards)
- [5. Related Open-Source Projects](#5-related-open-source-projects)
- [6. References](#6-references)

---

## 1. Key Papers

| Year | Title | Model | Institution | Link |
|------|-------|-------|-------------|------|
| 2023 | LLaVA: Visual Instruction Tuning | LLaVA | Microsoft, UIUC | [arXiv](https://arxiv.org/abs/2304.08485) |
| 2023 | MiniGPT-4 | MiniGPT-4 | Vision-CAIR | [arXiv](https://arxiv.org/abs/2304.10592) |
| 2024 | mPLUG-Owl2 | mPLUG-Owl2 | DAMO Academy | [arXiv](https://arxiv.org/abs/2308.12966) |
| 2023 | Kosmos-2 | Kosmos | Microsoft | [arXiv](https://arxiv.org/abs/2306.14824) |
| 2024 | Gemini 1.5 | Gemini | Google DeepMind | [blog](https://deepmind.google/technologies/gemini/gemini-15/) |

> ✅ 최신 논문은 `/papers/` 디렉토리에 PDF로 수록되어 있습니다.

---

## 2. Training Datasets

| Dataset | Modalities | Size | Source | Link |
|---------|------------|------|--------|------|
| CC3M | Image-Text | 3M pairs | Google | [CC3M](https://github.com/google/cc3m) |
| LAION-5B | Image-Text | 5B pairs | LAION | [LAION](https://laion.ai/blog/laion-5b/) |
| Visual Genome | Image-Text | 108K images | Stanford | [VG](https://visualgenome.org/) |
| ChartQA | Chart-Text | 20K QA pairs | UW | [ChartQA](https://github.com/vis-nlp/ChartQA) |
| ScienceQA | Image+Text+Reasoning | 21K QA | CMU | [ScienceQA](https://github.com/lupantech/ScienceQA) |

---

## 3. Benchmarks

| Benchmark | Description | Task Type | Link |
|-----------|-------------|-----------|------|
| MMMU | Multimodal Math & Science Reasoning | Multi-choice QA | [MMMU](https://mmmu-benchmark.github.io/) |
| MathVista | Math Diagrams + QA | Diagram QA | [MathVista](https://mathvista.github.io/) |
| ChartQA | Chart Image Comprehension | Visual QA | [ChartQA](https://github.com/vis-nlp/ChartQA) |
| ScienceQA | Multimodal Science QA | Multichoice Reasoning | [ScienceQA](https://github.com/lupantech/ScienceQA) |
| MathBench | Text + Diagram + Equation Reasoning | Mixed | [MathBench](https://mathbench.github.io/) |

---

## 4. Leaderboards

- [LLaVA Leaderboard](https://llava-vl.github.io/)
- [MMMU Leaderboard](https://mmmu-benchmark.github.io/)
- [ChartQA Leaderboard](https://chartqa.github.io/)
- [HuggingFace Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open-llm-leaderboard)

---

## 5. Related Open-Source Projects

- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)
- [mPLUG-Owl2](https://github.com/X-PLUG/mPLUG-Owl)
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)
- [GPT4-Vision-Adapter](https://github.com/Adapter-Hub/GPT4-Vision-Adapter)

---

## 6. References

- Papers with Code: [https://paperswithcode.com/](https://paperswithcode.com/)
- HuggingFace Model Zoo: [https://huggingface.co/models](https://huggingface.co/models)
- ArXiv MLLM Trackers: [https://www.arxiv-sanity.com/](https://www.arxiv-sanity.com/)

---

💡 *이 저장소는 지속적으로 업데이트됩니다.*
